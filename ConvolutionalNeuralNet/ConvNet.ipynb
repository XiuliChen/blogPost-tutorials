{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = pd.read_csv(\"mnist-data/digits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10,28,28,3)\n",
    "w = np.random.randn(3,3,3,16)\n",
    "b = np.random.randn(1,1,1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(x,mode=\"max\"):\n",
    "    #m*n_c*w*h\n",
    "    x_patches = x.reshape(x.shape[0],x.shape[1]//2, 2,x.shape[2]//2, 2,x.shape[3])\n",
    "    if mode==\"max\":\n",
    "        out = x_patches.max(axis=2).max(axis=3)\n",
    "        mask  =np.isclose(x,np.repeat(np.repeat(out,2,axis=1),2,axis=2)).astype(int)\n",
    "    elif mode==\"average\": \n",
    "        out =  x_patches.mean(axis=3).mean(axis=4)\n",
    "        mask = np.ones_like(x)*0.25\n",
    "    return out,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dx, mask):\n",
    "    return mask*(np.repeat(np.repeat(dx,2,axis=1),2,axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, deriv=False):\n",
    "    if deriv:\n",
    "        return (x>0)\n",
    "    return np.multiply(x, x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(x,w,b,padding=\"same\"):\n",
    "    if padding==\"same\": \n",
    "        pad = (w.shape[0]-1)//2\n",
    "    else: #padding is valid - i.e no zero padding\n",
    "        pad =0 \n",
    "    n = (x.shape[1]-w.shape[0]+2*pad) +1 #ouput width/height\n",
    "    y = np.zeros((x.shape[0],n,n,w.shape[3]))\n",
    "    x = np.pad(x,((0,0),(pad,pad),(pad,pad),(0,0)),'constant', constant_values = 0)\n",
    "    w = np.flip(w,0)\n",
    "    w = np.flip(w,1)\n",
    "    \n",
    "    f = w.shape[0]\n",
    "        \n",
    "    for i in range(x.shape[0]):\n",
    "        for k in range(w.shape[3]):\n",
    "            for d in range(x.shape[3]):\n",
    "                y[i,:,:,k]+=ndimage.convolve(x[i,:,:,d],w[:,:,d,k])[f//2:-(f//2),f//2:-(f//2)]\n",
    "                #ndimage.convolve starts convolution from centre of kernel and zero pads but we don't want this\n",
    "                #since we want to manually decide if we want to pad or not\n",
    "    y = y + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_forward(x,w,b):\n",
    "    return relu(w.dot(x)+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward(dA,z,x,w,b):\n",
    "    m = dA.shape[1]\n",
    "    dZ = dA*relu(z, deriv = True)\n",
    "    dW = (1/m)*dZ.dot(x.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dx =  np.dot(w.T,dZ)\n",
    "    return dx, dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward(x,w,b):\n",
    "    z = w.dot(x)+b\n",
    "    z-= np.mean(z,axis=0)\n",
    "    return np.exp(z)/np.sum(np.exp(z),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(y_hat, y, w, b, x):\n",
    "    m = y.shape[1]\n",
    "    dZ = y_hat - y\n",
    "    dW = (1/m)*dZ.dot(x.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dx =  np.dot(w.T,dZ)\n",
    "    return dx, dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ,x,w,padding=\"same\"):\n",
    "    m = x.shape[0]\n",
    "    db = (1/m)*np.sum(dZ, axis=(0,1,2), keepdims=True)\n",
    "    \n",
    "    if padding==\"same\": \n",
    "        pad = (w.shape[0]-1)//2\n",
    "    else: #padding is valid - i.e no zero padding\n",
    "        pad =0 \n",
    "    x_padded = np.pad(x,((0,0),(pad,pad),(pad,pad),(0,0)),'constant', constant_values = 0)\n",
    "    \n",
    "    #this will allow us to broadcast operation\n",
    "    x_padded_bcast = np.expand_dims(x_padded, axis=-1) \n",
    "    dZ_bcast = np.expand_dims(dZ, axis=-2)\n",
    "    \n",
    "    dW = np.zeros_like(w)\n",
    "    f=w.shape[0]\n",
    "    w_x = x_padded.shape[1]\n",
    "    for i in range(f):\n",
    "        for j in range(f):\n",
    "            dW[i,j,:,:] = (1/m)*np.sum(dZ_bcast*x_padded_bcast[:,i:w_x-(f-1 -i),j:w_x-(f-1 -j),:,:],axis=(0,1,2))  \n",
    "    \n",
    "    dx = np.ones_like(x_padded)*0.0\n",
    "    Z_pad = f-1\n",
    "    dZ_padded = np.pad(dZ,((0,0),(Z_pad,Z_pad),(Z_pad,Z_pad),(0,0)),'constant', constant_values = 0)  \n",
    "    for i in range(x.shape[0]):\n",
    "        for k in range(w.shape[3]):\n",
    "            for d in range(x.shape[3]):\n",
    "                dx[i,:,:,d]+=ndimage.convolve(dZ_padded[i,:,:,k],w[:,:,d,k])[f//2:-(f//2),f//2:-(f//2)]\n",
    "    dx = dx[:,pad:dx.shape[1]-pad,pad:dx.shape[2]-pad,:]\n",
    "    return dx,dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to define the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_conv_parameters(f, n_c, k):\n",
    "    return 0.001*np.random.rand(f,f,n_c,k), np.random.rand(1,1,1,k)+1\n",
    "def init_fc_parameters(n_x,n_y):\n",
    "    return 0.001*np.random.rand(n_y,n_x),np.random.rand(n_y,1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parameters():    \n",
    "    parameters={}\n",
    "    parameters[\"W_conv1\"], parameters[\"b_conv1\"] = init_conv_parameters(5, 1, 16)\n",
    "    #parameters[\"W_conv2\"], parameters[\"b_conv2\"] = init_conv_parameters(3, 16, 16)\n",
    "\n",
    "   # parameters[\"W_conv3\"], parameters[\"b_conv3\"] = init_conv_parameters(3, 16, 32)\n",
    "   # parameters[\"W_conv4\"], parameters[\"b_conv4\"] = init_conv_parameters(3, 32,32)\n",
    "\n",
    "    parameters[\"W_fc1\"],parameters[\"b_fc1\"] = init_fc_parameters(3136,512)\n",
    "    parameters[\"W_softmax\"],parameters[\"b_softmax\"] = init_fc_parameters(512,10)\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,parameters):\n",
    "    \n",
    "    cache={}\n",
    "    \n",
    "    cache[\"z_conv1\"] = conv_forward(X,parameters[\"W_conv1\"], parameters[\"b_conv1\"])\n",
    "    cache[\"a_conv1\"] = relu(cache[\"z_conv1\"])\n",
    "\n",
    "    #cache[\"z_conv2\"] = conv_forward(cache[\"a_conv1\"],parameters[\"W_conv2\"], parameters[\"b_conv2\"])\n",
    "    #cache[\"a_conv2\"] = relu(cache[\"z_conv2\"])\n",
    "\n",
    "   # cache[\"z_pool1\"], cache[\"mask_pool1\"] = pool_forward(cache[\"a_conv2\"])\n",
    "    \n",
    " \n",
    "\n",
    "   # cache[\"z_conv3\"] = conv_forward(cache[\"z_pool1\"],parameters[\"W_conv3\"], parameters[\"b_conv3\"])\n",
    "   # cache[\"a_conv3\"] = relu(cache[\"z_conv3\"])\n",
    "    \n",
    " \n",
    "\n",
    "   # cache[\"z_conv4\"] = conv_forward(cache[\"a_conv3\"],parameters[\"W_conv4\"], parameters[\"b_conv4\"] )\n",
    "  #  cache[\"a_conv4\"] = relu(cache[\"z_conv4\"])\n",
    "    \n",
    " \n",
    "  #  cache[\"z_pool2\"], cache[\"mask_pool2\"] = pool_forward(cache[\"a_conv4\"])\n",
    "    cache[\"z_pool2\"], cache[\"mask_pool2\"] = pool_forward(cache[\"a_conv1\"])\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    cache[\"a_flatten\"] = np.reshape(cache[\"z_pool2\"], (cache[\"z_pool2\"].shape[0],-1)).T\n",
    "    #cache[\"a_flatten\"] = np.reshape(cache[\"a_conv2\"], (cache[\"a_conv2\"].shape[0],-1)).T\n",
    "\n",
    "\n",
    " \n",
    "    cache[\"a_fc1\"] = fc_forward( cache[\"a_flatten\"],parameters[\"W_fc1\"],parameters[\"b_fc1\"])\n",
    "    \n",
    " \n",
    "    return softmax_forward(cache[\"a_fc1\"],parameters[\"W_softmax\"],parameters[\"b_softmax\"]),cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X,Y,Y_pred,parameters,cache,lambd):\n",
    "    grads = {}\n",
    "    \n",
    "    dA, grads[\"dW_softmax\"],grads[\"db_softmax\"] =softmax_backward(Y_pred, Y, parameters[\"W_softmax\"],parameters[\"b_softmax\"],cache[\"a_fc1\"])\n",
    "\n",
    "    dA, grads[\"dW_fc1\"],grads[\"db_fc1\"] = fc_backward(dA,cache[\"a_fc1\"],cache[\"a_flatten\"],parameters[\"W_fc1\"],parameters[\"b_fc1\"])\n",
    "    \n",
    "    dA = np.reshape(dA,cache[\"z_pool2\"].shape)\n",
    "    dA = pool_backward(dA, cache[\"mask_pool2\"])\n",
    "    \n",
    "    #dA = dA*relu(cache[\"z_conv3\"],deriv=True)\n",
    "   # dA, grads[\"dW_conv4\"],grads[\"db_conv4\"] = conv_backward(dA,cache[\"a_conv3\"],parameters[\"W_conv4\"])\n",
    "    \n",
    "   # dA = dA*relu(cache[\"z_conv3\"],deriv=True)\n",
    "   # dA, grads[\"dW_conv3\"],grads[\"db_conv3\"] = conv_backward(dA,cache[\"z_pool1\"],parameters[\"W_conv3\"])\n",
    "    \n",
    "   # dA = pool_backward(dA, cache[\"mask_pool1\"])\n",
    "    \n",
    "    #dA = dA*relu(cache[\"z_conv2\"],deriv=True)\n",
    "    #dA, grads[\"dW_conv2\"],grads[\"db_conv2\"] = conv_backward(dA,cache[\"a_conv1\"],parameters[\"W_conv2\"])\n",
    "    \n",
    "    dA = dA*relu(cache[\"z_conv1\"],deriv=True)\n",
    "    _, grads[\"dW_conv1\"],grads[\"db_conv1\"] = conv_backward(dA,X,parameters[\"W_conv1\"])\n",
    "    \n",
    "    #regularisation term\n",
    "    for key in grads:\n",
    "        if \"W\" in key:\n",
    "            grads[key]+= (lambd/X.shape[0])*parameters[key[1:]] \n",
    "    return grads        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred,y,parameters,lambd):\n",
    "    cost = 0\n",
    "    m = y.shape[1]\n",
    "    L = len(parameters)//2\n",
    "    cost += (-1/m)*np.sum(y*np.log(y_pred))\n",
    "    \n",
    "    regularisation_term = 0\n",
    "    for key in parameters:\n",
    "        if \"W\" in key:\n",
    "            regularisation_term += np.sum(np.square(parameters[key]))\n",
    "    \n",
    "    regularised_cost = cost + (lambd/(2*m))*regularisation_term\n",
    "    \n",
    "    return regularised_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y):\n",
    "    preds = np.argmax(y_pred,axis=0)\n",
    "    truth = np.argmax(y,axis=0)\n",
    "    return np.mean(np.equal(preds,truth).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_dev, Y_dev,num_epochs,batch_size,lambd,learning_rate):\n",
    "    train_costs = []\n",
    "    dev_evals = []\n",
    "    \n",
    "    parameters = initialise_parameters()\n",
    "    for epoch in range (num_epochs):\n",
    "        print(\"Training the model, epoch: \" + str(epoch+1))\n",
    "        #cycle through the entire training set in batches\n",
    "        for i in range(0,X_train.shape[0]//batch_size):\n",
    "            \n",
    "            \n",
    "            #get the next minibatch to train on\n",
    "            X_train_minibatch = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            Y_train_minibatch = Y_train[:,i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #perform one cycle of forward and backward propagation to get the partial derivatives w.r.t. the weights\n",
    "            #and biases. Calculate the cost - used to monitor training\n",
    "            y_pred, cache = forward_prop(X_train_minibatch,parameters)\n",
    "            minibatch_cost = loss_function(y_pred,Y_train_minibatch,parameters,lambd)\n",
    "            minibatch_grads = backprop(X_train_minibatch,Y_train_minibatch,y_pred,parameters, cache,lambd)\n",
    "            \n",
    "            \n",
    "            #update the parameters using gradient descent\n",
    "            for param in parameters.keys():\n",
    "                parameters[param] = parameters[param] - learning_rate*minibatch_grads[\"d\"+param]\n",
    "            \n",
    "            train_costs.append(minibatch_cost)\n",
    "            print(\"Cost after iteration \" + str(i*(epoch+1)) + \": \" + str(minibatch_cost))\n",
    "            #periodically output an update on the current cost and performance on the dev set for visualisation\n",
    "            if(i*(epoch+1)%1000 == 0):\n",
    "                \n",
    "                y_dev_pred,_ = forward_prop(X_dev,parameters)\n",
    "                dev_eval_metric = accuracy(y_dev_pred,Y_dev)\n",
    "                dev_evals.append(dev_eval_metric)\n",
    "                print(\"Accuracy on dev set: \"+ str(dev_eval_metric))\n",
    "    print(\"Training complete!\")\n",
    "    #return the trained parameters and the visualisation metrics\n",
    "    return parameters, train_costs, dev_evals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(train_costs, dev_evals, parameters,X_train, Y_train, X_dev, Y_dev, X_test, Y_test):\n",
    "    #plot the graphs of training set error and dev set F1 score\n",
    "    plt.plot(np.squeeze(train_costs))\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('1000s of Iterations')\n",
    "    plt.title(\"Training Set Error\")\n",
    "    plt.show()\n",
    "    plt.plot(np.squeeze(dev_evals))\n",
    "    plt.ylabel('F1 score')\n",
    "    plt.xlabel('1000s of Iterations')\n",
    "    plt.title(\"Dev Set Accuracy\")\n",
    "    plt.show()\n",
    "        \n",
    "    #For each of the train, dev and test sets, perform a step of forward propagation to obtain the trained model's \n",
    "    #predictions and evaluate this with an F1 score.\n",
    "    y_pred_train,_ = forward_prop(X_train,parameters)\n",
    "    print(\"The train set accuracy is: \"+str(accuracy(y_pred_train,Y_train)))\n",
    "    \n",
    "    y_pred_dev,_ = forward_prop(X_dev,parameters)\n",
    "    print(\"The dev set accuracy is: \"+str(accuracy(y_pred_dev,Y_dev)))\n",
    "    \n",
    "    y_pred_test,_= forward_prop(X_test,parameters)\n",
    "    print(\"The test set F1 score is: \"+str(accuracy(y_pred_test,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(TrainTestDevSets, hyperparameters):\n",
    "    \n",
    "    X_train = TrainTestDevSets[\"X_train\"] \n",
    "    Y_train = TrainTestDevSets[\"Y_train\"]\n",
    "\n",
    "    X_dev = TrainTestDevSets[\"X_dev\"]\n",
    "    Y_dev = TrainTestDevSets[\"Y_dev\"] \n",
    "    \n",
    "    X_test = TrainTestDevSets[\"X_test\"] \n",
    "    Y_test = TrainTestDevSets[\"Y_test\"]\n",
    "    \n",
    "    num_epochs = hyperparameters[\"num_epochs\"]\n",
    "    batch_size = hyperparameters[\"batch_size\"]\n",
    "    lambd = hyperparameters[\"lambd\"]\n",
    "    learning_rate = hyperparameters[\"learning_rate\"]\n",
    "    \n",
    "    parameters, train_costs, dev_evals = train_model(X_train, Y_train, X_dev, Y_dev,num_epochs,batch_size,lambd,learning_rate)         \n",
    "    evaluate_model(train_costs, dev_evals, parameters,X_train, Y_train, X_dev, Y_dev, X_test, Y_test)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = mnist_data.reindex(np.random.permutation(mnist_data.index)) #shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_data.loc[:,mnist_data.columns!=\"label\"].as_matrix()\n",
    "X = np.reshape(X,(X.shape[0], 28,28,1))\n",
    "Y = mnist_data.loc[:,[\"label\"]].as_matrix()\n",
    "Y = np.eye(10)[Y.reshape(-1)].T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TrainTestDevSets={}\n",
    "TrainTestDevSets[\"X_train\"] = X[:28000]\n",
    "TrainTestDevSets[\"Y_train\"] = Y[:,:28000]\n",
    "TrainTestDevSets[\"X_dev\"] = X[28000:35000]\n",
    "TrainTestDevSets[\"Y_dev\"] =Y[:,28000:35000]\n",
    "TrainTestDevSets[\"X_test\"] = X[35000:]\n",
    "TrainTestDevSets[\"Y_test\"] = Y[:,35000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={}\n",
    "hyperparameters[\"num_epochs\"] = 1 #number of passes through the training set\n",
    "hyperparameters[\"batch_size\"] = 128 #number of examples trained upon in each step of training\n",
    "hyperparameters[\"lambd\"] = 1 #regularisation parameter \n",
    "hyperparameters[\"learning_rate\"] = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model, epoch: 1\n",
      "Cost after iteration 0: 2.342667867674307\n",
      "Accuracy on dev set: 0.105\n",
      "Cost after iteration 1: 24.215532560797165\n",
      "Cost after iteration 2: 14.875682895017652\n",
      "Cost after iteration 3: 2.3579929470464407\n",
      "Cost after iteration 4: 2.365244440330007\n",
      "Cost after iteration 5: 2.3970116707316675\n",
      "Cost after iteration 6: 2.331258868315088\n",
      "Cost after iteration 7: 2.3822838967754913\n",
      "Cost after iteration 8: 2.3272779840968574\n",
      "Cost after iteration 9: 2.2955731875519234\n",
      "Cost after iteration 10: 2.4148770354467066\n",
      "Cost after iteration 11: 2.363066433212982\n",
      "Cost after iteration 12: 2.3218673023813103\n",
      "Cost after iteration 13: 2.349211263797381\n",
      "Cost after iteration 14: 2.3083426863907457\n",
      "Training the model, epoch: 2\n",
      "Cost after iteration 0: 2.3467546109391275\n",
      "Accuracy on dev set: 0.125\n",
      "Cost after iteration 2: 2.3227183181087536\n",
      "Cost after iteration 4: 2.345983948346587\n",
      "Cost after iteration 6: 2.342568150146759\n",
      "Cost after iteration 8: 2.355557468674453\n",
      "Cost after iteration 10: 2.3860516590725314\n",
      "Cost after iteration 12: 2.322856327894436\n",
      "Cost after iteration 14: 2.37327698598202\n",
      "Cost after iteration 16: 2.322881678366133\n",
      "Cost after iteration 18: 2.29089837883188\n",
      "Cost after iteration 20: 2.3613573091906304\n",
      "Cost after iteration 22: 2.349476229633032\n",
      "Cost after iteration 24: 2.3154627700958166\n",
      "Cost after iteration 26: 2.3414718246389374\n",
      "Cost after iteration 28: 2.29689471166497\n",
      "Training the model, epoch: 3\n",
      "Cost after iteration 0: 2.339804724244258\n",
      "Accuracy on dev set: 0.125\n",
      "Cost after iteration 3: 2.312036809768043\n",
      "Cost after iteration 6: 2.332894486123238\n",
      "Cost after iteration 9: 2.326720331775456\n",
      "Cost after iteration 12: 2.329567409669248\n",
      "Cost after iteration 15: 2.3660775708300186\n",
      "Cost after iteration 18: 2.309952093670157\n",
      "Cost after iteration 21: 2.348978164728813\n",
      "Cost after iteration 24: 2.3083740464208935\n",
      "Cost after iteration 27: 2.2734972043395496\n",
      "Cost after iteration 30: 2.3185634936183126\n",
      "Cost after iteration 33: 2.309290918629775\n",
      "Cost after iteration 36: 2.297869686455552\n",
      "Cost after iteration 39: 2.3200062451535417\n",
      "Cost after iteration 42: 2.264223160812983\n",
      "Training the model, epoch: 4\n",
      "Cost after iteration 0: 2.2959189080214677\n",
      "Accuracy on dev set: 0.125\n",
      "Cost after iteration 4: 2.2730914661290766\n",
      "Cost after iteration 8: 2.2606051179369526\n",
      "Cost after iteration 12: 2.309611218239071\n",
      "Cost after iteration 16: 2.3268215994162293\n",
      "Cost after iteration 20: 2.321092772307096\n",
      "Cost after iteration 24: 2.2482626299615154\n",
      "Cost after iteration 28: 2.2746179099271178\n",
      "Cost after iteration 32: 2.2654502369604073\n",
      "Cost after iteration 36: 2.2270744433698617\n",
      "Cost after iteration 40: 2.2595761531591725\n",
      "Cost after iteration 44: 2.2076990041184508\n",
      "Cost after iteration 48: 2.2565420028187186\n",
      "Cost after iteration 52: 2.248874942975865\n",
      "Cost after iteration 56: 2.191979156470313\n",
      "Training the model, epoch: 5\n",
      "Cost after iteration 0: 2.2284285614671915\n",
      "Accuracy on dev set: 0.22\n",
      "Cost after iteration 5: 2.177419607513171\n",
      "Cost after iteration 10: 2.153031898835634\n",
      "Cost after iteration 15: 2.261593664137895\n",
      "Cost after iteration 20: 2.2948921308162236\n",
      "Cost after iteration 25: 2.22116482389389\n",
      "Cost after iteration 30: 2.1905243694373935\n",
      "Cost after iteration 35: 2.196255116714774\n",
      "Cost after iteration 40: 2.2168892108593736\n",
      "Cost after iteration 45: 2.1535031645883653\n",
      "Cost after iteration 50: 2.199503711609025\n",
      "Cost after iteration 55: 2.111137274110936\n",
      "Cost after iteration 60: 2.1989481906653072\n",
      "Cost after iteration 65: 2.180225860361427\n",
      "Cost after iteration 70: 2.1197990144630516\n",
      "Training the model, epoch: 6\n",
      "Cost after iteration 0: 2.1699207192013765\n",
      "Accuracy on dev set: 0.215\n",
      "Cost after iteration 6: 2.1038611909459557\n",
      "Cost after iteration 12: 2.0737831812328844\n",
      "Cost after iteration 18: 2.2098351579897133\n",
      "Cost after iteration 24: 2.234147349834565\n",
      "Cost after iteration 30: 2.1296424467222814\n",
      "Cost after iteration 36: 2.1270237181342293\n",
      "Cost after iteration 42: 2.13970996346355\n",
      "Cost after iteration 48: 2.184457826292781\n",
      "Cost after iteration 54: 2.1209573441693665\n",
      "Cost after iteration 60: 2.1579856679584317\n",
      "Cost after iteration 66: 2.0616261930389954\n",
      "Cost after iteration 72: 2.156511552340722\n",
      "Cost after iteration 78: 2.119407896435103\n",
      "Cost after iteration 84: 2.0611271934060396\n",
      "Training the model, epoch: 7\n",
      "Cost after iteration 0: 2.136456269432621\n",
      "Accuracy on dev set: 0.22\n",
      "Cost after iteration 7: 2.054414839735736\n",
      "Cost after iteration 14: 2.023565049474659\n",
      "Cost after iteration 21: 2.1573979894609323\n",
      "Cost after iteration 28: 2.217389402191542\n",
      "Cost after iteration 35: 2.0727596923175904\n",
      "Cost after iteration 42: 2.0715899663965276\n",
      "Cost after iteration 49: 2.0775587462016247\n",
      "Cost after iteration 56: 2.1252582304848695\n",
      "Cost after iteration 63: 2.059765056645138\n",
      "Cost after iteration 70: 2.1135205112104494\n",
      "Cost after iteration 77: 2.0077699475559965\n",
      "Cost after iteration 84: 2.1007789318049372\n",
      "Cost after iteration 91: 2.0471219459384713\n",
      "Cost after iteration 98: 1.9851187301773\n",
      "Training the model, epoch: 8\n",
      "Cost after iteration 0: 2.083608590699864\n",
      "Accuracy on dev set: 0.23\n",
      "Cost after iteration 8: 1.9957579149375984\n",
      "Cost after iteration 16: 1.9654333290323556\n",
      "Cost after iteration 24: 2.08679237958317\n",
      "Cost after iteration 32: 2.1402525965227692\n",
      "Cost after iteration 40: 2.0062153493471833\n",
      "Cost after iteration 48: 2.0028930236927467\n",
      "Cost after iteration 56: 2.0036941474577388\n",
      "Cost after iteration 64: 2.032437443278899\n",
      "Cost after iteration 72: 1.9575192545197577\n",
      "Cost after iteration 80: 2.046577113474442\n",
      "Cost after iteration 88: 1.9330957943777414\n",
      "Cost after iteration 96: 2.0259410195447267\n",
      "Cost after iteration 104: 1.9623768081920374\n",
      "Cost after iteration 112: 1.8840691050681564\n",
      "Training the model, epoch: 9\n",
      "Cost after iteration 0: 2.0049926313186583\n",
      "Accuracy on dev set: 0.3\n",
      "Cost after iteration 9: 1.9196576752738124\n",
      "Cost after iteration 18: 1.877854317451305\n",
      "Cost after iteration 27: 1.9965064133530532\n",
      "Cost after iteration 36: 2.0028812413576413\n",
      "Cost after iteration 45: 1.9167505967629306\n",
      "Cost after iteration 54: 1.9107954390240924\n",
      "Cost after iteration 63: 1.9101707695550085\n",
      "Cost after iteration 72: 1.946872801072428\n",
      "Cost after iteration 81: 1.8760198830163062\n",
      "Cost after iteration 90: 1.963181402780258\n",
      "Cost after iteration 99: 1.8459047124163912\n",
      "Cost after iteration 108: 1.9277819019361537\n",
      "Cost after iteration 117: 1.858731591736527\n",
      "Cost after iteration 126: 1.7779716589138639\n",
      "Training the model, epoch: 10\n",
      "Cost after iteration 0: 1.9477746639168902\n",
      "Accuracy on dev set: 0.305\n",
      "Cost after iteration 10: 1.8806395221786711\n",
      "Cost after iteration 20: 1.8541636320723862\n",
      "Cost after iteration 30: 1.9095363894073365\n",
      "Cost after iteration 40: 1.8886090203332948\n",
      "Cost after iteration 50: 1.8212681302293487\n",
      "Cost after iteration 60: 1.8093105752933458\n",
      "Cost after iteration 70: 1.8246282253271506\n",
      "Cost after iteration 80: 1.8610210355698427\n",
      "Cost after iteration 90: 1.7890240653252611\n",
      "Cost after iteration 100: 1.8775681085095854\n",
      "Cost after iteration 110: 1.7583644920470716\n",
      "Cost after iteration 120: 1.8448774487212474\n",
      "Cost after iteration 130: 1.7848847339600185\n",
      "Cost after iteration 140: 1.7473962457642531\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "parameters =train_model(X[:2000],Y[:,:2000],X[2800:3000],Y[:,2800:3000],10,128,0,0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f14d828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.squeeze(parameters[1]))\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('1000s of Iterations')\n",
    "plt.title(\"Training Set Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
