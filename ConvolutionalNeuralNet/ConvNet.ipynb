{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = pd.read_csv(\"mnist-data/digits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(x,mode=\"max\"):\n",
    "    #m*n_c*w*h\n",
    "    x_patches = x.reshape(x.shape[0],x.shape[1]//2, 2,x.shape[2]//2, 2,x.shape[3])\n",
    "    if mode==\"max\":\n",
    "        out = x_patches.max(axis=2).max(axis=3)\n",
    "        mask  =np.isclose(x,np.repeat(np.repeat(out,2,axis=1),2,axis=2)).astype(int)\n",
    "    elif mode==\"average\": \n",
    "        out =  x_patches.mean(axis=3).mean(axis=4)\n",
    "        mask = np.ones_like(x)*0.25\n",
    "    return out,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dx, mask):\n",
    "    return mask*(np.repeat(np.repeat(dx,2,axis=1),2,axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, deriv=False):\n",
    "    if deriv:\n",
    "        return (x>0)\n",
    "    return np.multiply(x, x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(x,w,b,padding=\"same\"):\n",
    "    if padding==\"same\": \n",
    "        pad = (w.shape[0]-1)//2\n",
    "    else: #padding is valid - i.e no zero padding\n",
    "        pad =0 \n",
    "    n = (x.shape[1]-w.shape[0]+2*pad) +1 #ouput width/height\n",
    "    y = np.zeros((x.shape[0],n,n,w.shape[3]))\n",
    "    x = np.pad(x,((0,0),(pad,pad),(pad,pad),(0,0)),'constant', constant_values = 0)\n",
    "    w = np.flip(w,0)\n",
    "    w = np.flip(w,1)\n",
    "    \n",
    "    f = w.shape[0]\n",
    "        \n",
    "    for i in range(x.shape[0]):\n",
    "        for k in range(w.shape[3]):\n",
    "            for d in range(x.shape[3]):\n",
    "                y[i,:,:,k]+=ndimage.convolve(x[i,:,:,d],w[:,:,d,k])[f//2:-(f//2),f//2:-(f//2)]\n",
    "                #ndimage.convolve starts convolution from centre of kernel and zero pads but we don't want this\n",
    "                #since we want to manually decide if we want to pad or not\n",
    "    y = y + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_forward(x,w,b):\n",
    "    return relu(w.dot(x)+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward(dA,a,x,w,b):\n",
    "    m = dA.shape[1]\n",
    "    dZ = dA*relu(a, deriv = True)\n",
    "    dW = (1/m)*dZ.dot(x.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dx =  np.dot(w.T,dZ)\n",
    "    return dx, dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward(x,w,b):\n",
    "    z = w.dot(x)+b\n",
    "    a = np.exp(z)\n",
    "    a = a/np.sum(a,axis=0,keepdims=True)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(y_hat, y, w, b, x):\n",
    "    m = y.shape[1]\n",
    "    dZ = y_hat - y\n",
    "    dW = (1/m)*dZ.dot(x.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dx =  np.dot(w.T,dZ)\n",
    "    return dx, dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ,x,w,padding=\"same\"):\n",
    "    m = x.shape[0]\n",
    "    db = (1/m)*np.sum(dZ, axis=(0,1,2), keepdims=True)\n",
    "    \n",
    "    if padding==\"same\": \n",
    "        pad = (w.shape[0]-1)//2\n",
    "    else: #padding is valid - i.e no zero padding\n",
    "        pad =0 \n",
    "    x_padded = np.pad(x,((0,0),(pad,pad),(pad,pad),(0,0)),'constant', constant_values = 0)\n",
    "    \n",
    "    #this will allow us to broadcast operation\n",
    "    x_padded_bcast = np.expand_dims(x_padded, axis=-1) \n",
    "    dZ_bcast = np.expand_dims(dZ, axis=-2)\n",
    "    \n",
    "    dW = np.zeros_like(w)\n",
    "    f=w.shape[0]\n",
    "    w_x = x_padded.shape[1]\n",
    "    for i in range(f):\n",
    "        for j in range(f):\n",
    "            dW[i,j,:,:] = (1/m)*np.sum(dZ_bcast*x_padded_bcast[:,i:w_x-(f-1 -i),j:w_x-(f-1 -j),:,:],axis=(0,1,2))  \n",
    "    \n",
    "    dx = np.ones_like(x_padded)*0.0\n",
    "    Z_pad = f-1\n",
    "    dZ_padded = np.pad(dZ,((0,0),(Z_pad,Z_pad),(Z_pad,Z_pad),(0,0)),'constant', constant_values = 0)  \n",
    "    for i in range(x.shape[0]):\n",
    "        for k in range(w.shape[3]):\n",
    "            for d in range(x.shape[3]):\n",
    "                dx[i,:,:,d]+=ndimage.convolve(dZ_padded[i,:,:,k],w[:,:,d,k])[f//2:-(f//2),f//2:-(f//2)]\n",
    "    dx = dx[:,pad:dx.shape[1]-pad,pad:dx.shape[2]-pad,:]\n",
    "    return dx,dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to define the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_conv_parameters(f, n_c, k):\n",
    "    \n",
    "    return 0.001*np.random.rand(f,f,n_c,k), np.ones((1,1,1,k))\n",
    "                                                                      \n",
    "def init_fc_parameters(n_x,n_y):\n",
    "    return 0.001*np.random.rand(n_y,n_x),np.zeros((n_y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parameters():    \n",
    "    parameters={}\n",
    "    parameters[\"W_conv1\"], parameters[\"b_conv1\"] = init_conv_parameters(5, 1, 16)\n",
    "    parameters[\"W_conv2\"], parameters[\"b_conv2\"] = init_conv_parameters(3, 16, 16)\n",
    "\n",
    "    parameters[\"W_conv3\"], parameters[\"b_conv3\"] = init_conv_parameters(3, 16, 32)\n",
    "    parameters[\"W_conv4\"], parameters[\"b_conv4\"] = init_conv_parameters(3, 32,32)\n",
    "\n",
    "    parameters[\"W_fc1\"],parameters[\"b_fc1\"] = init_fc_parameters(1568,512)\n",
    "    parameters[\"W_softmax\"],parameters[\"b_softmax\"] = init_fc_parameters(512,10)\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,parameters):\n",
    "    \n",
    "    cache={}\n",
    "    \n",
    "    cache[\"z_conv1\"] = conv_forward(X,parameters[\"W_conv1\"], parameters[\"b_conv1\"])\n",
    "    cache[\"a_conv1\"] = relu(cache[\"z_conv1\"])\n",
    "\n",
    "    cache[\"z_conv2\"] = conv_forward(cache[\"a_conv1\"],parameters[\"W_conv2\"], parameters[\"b_conv2\"])\n",
    "    cache[\"a_conv2\"] = relu(cache[\"z_conv2\"])\n",
    "\n",
    "    cache[\"z_pool1\"], cache[\"mask_pool1\"] = pool_forward(cache[\"a_conv2\"])\n",
    "    \n",
    " \n",
    "\n",
    "    cache[\"z_conv3\"] = conv_forward(cache[\"z_pool1\"],parameters[\"W_conv3\"], parameters[\"b_conv3\"])\n",
    "    cache[\"a_conv3\"] = relu(cache[\"z_conv3\"])\n",
    "    \n",
    " \n",
    "\n",
    "    cache[\"z_conv4\"] = conv_forward(cache[\"a_conv3\"],parameters[\"W_conv4\"], parameters[\"b_conv4\"] )\n",
    "    cache[\"a_conv4\"] = relu(cache[\"z_conv4\"])\n",
    "    \n",
    " \n",
    "    cache[\"z_pool2\"], cache[\"mask_pool2\"] = pool_forward(cache[\"a_conv4\"])\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    cache[\"a_flatten\"] = np.reshape(cache[\"z_pool2\"], (cache[\"z_pool2\"].shape[0],-1)).T\n",
    "\n",
    " \n",
    "    cache[\"a_fc1\"] = fc_forward( cache[\"a_flatten\"],parameters[\"W_fc1\"],parameters[\"b_fc1\"])\n",
    "    \n",
    " \n",
    "    return softmax_forward(cache[\"a_fc1\"],parameters[\"W_softmax\"],parameters[\"b_softmax\"]),cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X,Y,Y_pred,parameters,cache,lambd):\n",
    "    grads = {}\n",
    "    \n",
    "    dA, grads[\"dW_softmax\"],grads[\"db_softmax\"] =softmax_backward(Y_pred, Y, parameters[\"W_softmax\"],parameters[\"b_softmax\"],cache[\"a_fc1\"])\n",
    "\n",
    "    dA, grads[\"dW_fc1\"],grads[\"db_fc1\"] = fc_backward(dA,cache[\"a_fc1\"],cache[\"a_flatten\"],parameters[\"W_fc1\"],parameters[\"b_fc1\"])\n",
    "    \n",
    "    dA = np.reshape(dA.T,cache[\"z_pool2\"].shape)\n",
    "    dA = pool_backward(dA, cache[\"mask_pool2\"])\n",
    "    \n",
    "    dA = dA*relu(cache[\"z_conv3\"],deriv=True)\n",
    "    dA, grads[\"dW_conv4\"],grads[\"db_conv4\"] = conv_backward(dA,cache[\"a_conv3\"],parameters[\"W_conv4\"])\n",
    "    \n",
    "    dA = dA*relu(cache[\"z_conv3\"],deriv=True)\n",
    "    dA, grads[\"dW_conv3\"],grads[\"db_conv3\"] = conv_backward(dA,cache[\"z_pool1\"],parameters[\"W_conv3\"])\n",
    "    \n",
    "    dA = pool_backward(dA, cache[\"mask_pool1\"])\n",
    "    \n",
    "    dA = dA*relu(cache[\"z_conv2\"],deriv=True)\n",
    "    dA, grads[\"dW_conv2\"],grads[\"db_conv2\"] = conv_backward(dA,cache[\"a_conv1\"],parameters[\"W_conv2\"])\n",
    "    \n",
    "    dA = dA*relu(cache[\"z_conv1\"],deriv=True)\n",
    "    _, grads[\"dW_conv1\"],grads[\"db_conv1\"] = conv_backward(dA,X,parameters[\"W_conv1\"])\n",
    "    \n",
    "    #regularisation term\n",
    "    for key in grads:\n",
    "        if \"W\" in key:\n",
    "            grads[key]+= (lambd/X.shape[0])*parameters[key[1:]] \n",
    "    return grads        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred,y,parameters,lambd):\n",
    "    m = y.shape[1]\n",
    "    cost = (-1/m)*np.sum(y*np.log(y_pred))\n",
    "    \n",
    "    regularisation_term = 0\n",
    "    for key in parameters:\n",
    "        if \"W\" in key:\n",
    "            regularisation_term += np.sum(np.square(parameters[key]))\n",
    "    \n",
    "    regularised_cost = cost + (lambd/(2*m))*regularisation_term\n",
    "    \n",
    "    return regularised_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y):\n",
    "    preds = np.argmax(y_pred,axis=0)\n",
    "    truth = np.argmax(y,axis=0)\n",
    "    return np.mean(np.equal(preds,truth).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def train_model(X_train, Y_train, X_dev, Y_dev,num_epochs,batch_size,lambd,learning_rate,parameters = initialise_parameters() ):\n",
    "    train_costs = []\n",
    "    train_evals = []\n",
    "    dev_evals = []\n",
    "    fig, (ax1, ax2,ax3) = plt.subplots(1,3,figsize=(10, 3))\n",
    "    \n",
    "    ax1.set_xlabel('Number of iterations')\n",
    "    ax1.set_ylabel('Error')\n",
    "    ax1.set_title('Training Set Error')\n",
    "    \n",
    "    ax2.set_xlabel('Number of iterations')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training Set Accuracy')\n",
    "    \n",
    "    ax3.set_xlabel('Number of iterations')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_title('Dev Set Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ion()\n",
    "\n",
    "    fig.show()\n",
    "    fig.canvas.draw()\n",
    "    for epoch in range (num_epochs):\n",
    "        print(\"Training the model, epoch: \" + str(epoch+1))\n",
    "        #cycle through the entire training set in batches\n",
    "        for i in range(0,X_train.shape[0]//batch_size):\n",
    "            \n",
    "            \n",
    "            #get the next minibatch to train on\n",
    "            X_train_minibatch = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            Y_train_minibatch = Y_train[:,i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #perform one cycle of forward and backward propagation to get the partial derivatives w.r.t. the weights\n",
    "            #and biases. Calculate the cost - used to monitor training\n",
    "            y_pred, cache = forward_prop(X_train_minibatch,parameters)\n",
    "            minibatch_cost = loss_function(y_pred,Y_train_minibatch,parameters,lambd)\n",
    "            minibatch_grads = backprop(X_train_minibatch,Y_train_minibatch,y_pred,parameters, cache,lambd)\n",
    "            \n",
    "            \n",
    "            #update the parameters using gradient descent\n",
    "            for param in parameters.keys():\n",
    "                parameters[param] = parameters[param] - learning_rate*minibatch_grads[\"d\"+param]\n",
    "            \n",
    "            train_costs.append(minibatch_cost)\n",
    "            ax1.plot(train_costs)\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "            \n",
    "            train_eval_metric = accuracy(y_pred,Y_train_minibatch)\n",
    "            train_evals.append(train_eval_metric)\n",
    "            ax2.plot(train_evals)\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "            #periodically output an update on the current cost and performance on the dev set for visualisation\n",
    "            if(i%100 == 0):\n",
    "                print(\"Training set error: \"+ str(minibatch_cost))\n",
    "                print(\"Training set accuracy: \"+ str(train_eval_metric))\n",
    "                y_dev_pred,_ = forward_prop(X_dev,parameters)\n",
    "                dev_eval_metric = accuracy(y_dev_pred,Y_dev)\n",
    "                dev_evals.append(dev_eval_metric)\n",
    "                print(\"Accuracy on dev set: \"+ str(dev_eval_metric))\n",
    "                ax3.plot(dev_evals)\n",
    "                fig.canvas.draw()\n",
    "    print(\"Training complete!\")\n",
    "    #return the trained parameters \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = mnist_data.reindex(np.random.permutation(mnist_data.index)) #shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_data.loc[:,mnist_data.columns!=\"label\"].values\n",
    "X = np.reshape(X,(X.shape[0], 28,28,1))\n",
    "X= X/255 #normalise input features \n",
    "Y = mnist_data.loc[:,[\"label\"]].values\n",
    "Y = np.eye(10)[Y.reshape(-1)].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters =train_model(X[:10000],Y[:,:10000],X[28000:28500],Y[:,28000:28500],\n",
    "                        num_epochs=1,batch_size=64,lambd=5,learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(forward_prop(X[-1000:],parameters)[0],Y[:,-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
