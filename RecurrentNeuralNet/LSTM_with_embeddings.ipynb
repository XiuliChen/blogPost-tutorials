{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmMXWSgsz937"
   },
   "source": [
    "### LSTM Network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1h1ccLEzWAm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm  import tqdm_notebook #used for loading bar (e.g. to visualise how long an operation is taking)\n",
    "\n",
    "#read in data\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(data_url, path):\n",
    "    if os.path.exists(os.getcwd()+\"/\"+path):\n",
    "        print(\"Data already present.\")\n",
    "    else:\n",
    "        print(\"Downloading data...\")\n",
    "        r = requests.get(data_url, stream=True)\n",
    "        file_name = data_url.split(\"/\")[-1]\n",
    "        with open(file_name,'wb') as f:\n",
    "            total_length = int(r.headers.get('content-length'))\n",
    "            for chunk in tqdm_notebook(r.iter_content(chunk_size=1024), total=(total_length/1024) + 1): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        print(\"Extracting data\")\n",
    "        if \"tar\" in file_name:\n",
    "            unzipper = tarfile.open(file_name)\n",
    "        elif \"zip\" in file_name:\n",
    "            unzipper = zipfile.ZipFile(file_name,\"r\")\n",
    "        unzipper.extractall()\n",
    "        unzipper.close()\n",
    "        os.remove(file_name) \n",
    "        print(\"Success: Data downloaded and extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a1b4fcc0a04327ad94b0d8fb7a6b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=82155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting data\n",
      "Success: Data downloaded and extracted.\n"
     ]
    }
   ],
   "source": [
    "maybe_download(data_url=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", path=\"aclImdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44b9f3521794adaab44aeede548b185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=841976), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maybe_download(data_url=url=\"http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\", path=\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_stats(X):\n",
    "    lengths = []\n",
    "    for x in X:\n",
    "        lengths.append(len(x))\n",
    "    plt.hist(lengths,bins=25)\n",
    "    plt.show()\n",
    "length_stats(X_train)\n",
    "\n",
    "max_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymiVKtTb0s4z"
   },
   "outputs": [],
   "source": [
    "idx_to_word = {(v+3):k for k,v in imdb.get_word_index().items()}\n",
    "idx_to_word.update({0:\"<PAD>\", 1: \"<START>\", 2: \"<UNK>\",3:\"<UNUSED>\"}) #first 3 indices are special tokens \n",
    "\n",
    "vocab_size = np.max(list(idx_to_word.keys()))\n",
    "#this is a helper function - good to debug performance of model during training\n",
    "def print_review(x):\n",
    "    text = \"\"\n",
    "    for idx in x:\n",
    "        text += idx_to_word.get(idx, \"<UNK>\") + \" \" #if word not in dictionary it is unknown\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DvoUYEonzWAw"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def initialise_parameters(n_a,n_x):\n",
    "    Wg, bg = np.random.rand(3*n_a,n_a+n_x), np.random.rand(3*n_a, 1)\n",
    "    Wc, bc = np.random.rand(n_a,n_a+n_x), np.random.rand(n_a, 1)\n",
    "    Wy, by = np.random.rand(1,n_a), np.random.rand(1, 1)\n",
    "    a0 = c0 = np.zeros((n_a,1))\n",
    "    parameters = {\"Wg\": Wg,\"Wc\": Wc,\"Wy\": Wy,\"bg\": bg, \"bc\": bc,\"by\": by, \"a0\": a0, \"c0\":c0}\n",
    "    return parameters\n",
    "    \n",
    "def forward_step(a_prev, x, c_prev, parameters):\n",
    "    n_a = a_prev.shape[0]\n",
    "    input_concat = np.concatenate((a_prev, x),axis=0)\n",
    "    IFO_gates = sigmoid(parameters[\"Wg\"].dot(input_concat)+parameters[\"bg\"])\n",
    "    c_candidate = np.tanh(parameters[\"Wc\"].dot(input_concat)+parameters[\"bc\"])\n",
    "    c_next = IFO_gates[:n_a]*c_candidate + IFO_gates[n_a:2*n_a]*c_prev\n",
    "    a_next = IFO_gates[2*n_a:]*np.tanh(c_next)\n",
    "    cache = (a_next, c_next, input_concat, c_prev, c_candidate,IFO_gates)\n",
    "    return a_next, c_next, cache\n",
    "\n",
    "\n",
    "def backward_step(dA_next, dC_next,cache,parameters):\n",
    "    (a_next, c_next, input_concat, c_prev, c_candidate,IFO_gates) = cache\n",
    "    n_a, m = a_next.shape\n",
    "    \n",
    "    dC_next += dA_next* (IFO_gates[2*n_a:]*(1-np.tanh(c_next)**2))\n",
    "    \n",
    "    dC_prev = dC_next * IFO_gates[n_a:2*n_a]\n",
    "    dC_candidate =  dC_next * IFO_gates[:n_a]\n",
    "    \n",
    "    dIFO_gates = np.zeros_like(IFO_gates)\n",
    "    dIFO_gates[:n_a] = dC_next * c_candidate \n",
    "    dIFO_gates[n_a:2*n_a]= dC_next * c_prev\n",
    "    dIFO_gates[2*n_a:] = dA_next * np.tanh(c_next)\n",
    "\n",
    "    dZ_gate =  dIFO_gates* (IFO_gates*(1-IFO_gates))   \n",
    "    dA_prev =  (parameters[\"Wg\"].T).dot(dZ_gate)[:n_a]\n",
    "    dWg = (1/m)*dZ_gate.dot(input_concat.T)\n",
    "    dbg = (1/m)*np.sum(dZ_gate,axis=1, keepdims=True)\n",
    "    \n",
    "    dZ_c = dC_candidate * (1-c_candidate**2)\n",
    "    dA_prev +=  (parameters[\"Wc\"].T).dot(dZ_c)[:n_a]\n",
    "    dWc = (1/m)*dZ_c.dot(input_concat.T)\n",
    "    dbc = (1/m)*np.sum(dZ_c,axis=1, keepdims=True)  \n",
    "    \n",
    "    return dA_prev, dC_prev, dWg, dbg, dWc, dbc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S61KBip-zWAz"
   },
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    m, _, Tx = X.shape\n",
    "    a_prev = np.repeat(parameters[\"a0\"],m, axis=1)\n",
    "    c_prev = np.repeat(parameters[\"c0\"],m, axis=1)\n",
    "    caches= []\n",
    "    for t in range(Tx):\n",
    "        xt = np.squeeze(X[:,:,t]).T #so 2D matrix n_x, m\n",
    "        a_prev, c_prev, cache = forward_step(a_prev,xt , c_prev, parameters)\n",
    "        caches.append(cache)\n",
    "    y_pred = sigmoid(parameters[\"Wy\"].dot(a_prev)+parameters[\"by\"])\n",
    "    return y_pred, caches\n",
    "\n",
    "\n",
    "def backprop(X,y, y_pred, parameters, caches,lambd):\n",
    "    m, _, Tx = X.shape\n",
    "    grads = {}\n",
    "    \n",
    "    \n",
    "    dZ_pred = (y_pred-y) \n",
    "    dA =  (parameters[\"Wy\"].T).dot(dZ_pred)\n",
    "    grads[\"dWy\"] = (1/m)*dZ_pred.dot(caches[Tx-1][0].T) #this is A<t>\n",
    "    grads[\"dby\"] = (1/m)*np.sum(dZ_pred,axis=1, keepdims=True)\n",
    "    \n",
    "    dC = np.zeros_like(dA)\n",
    "    \n",
    "    grads[\"dWg\"] = np.zeros_like(parameters[\"Wg\"])\n",
    "    grads[\"dbg\"] = np.zeros_like(parameters[\"bg\"])\n",
    "    grads[\"dWc\"] = np.zeros_like(parameters[\"Wc\"])\n",
    "    grads[\"dbc\"] = np.zeros_like(parameters[\"bc\"])\n",
    "    \n",
    "    \n",
    "    for t in reversed(range(Tx)):\n",
    "        (dA, dC, dWg, dbg, dWc, dbc) = backward_step(dA, dC, caches[t],parameters)\n",
    "        grads[\"dWg\"] += dWg\n",
    "        grads[\"dbg\"] += dbg\n",
    "        grads[\"dWc\"] += dWc\n",
    "        grads[\"dbc\"] += dbc\n",
    "    \n",
    "    grads[\"da0\"] = (1/m)*np.sum(dA,axis=1,keepdims=True)\n",
    "    grads[\"dc0\"] = (1/m)*np.sum(dC,axis=1,keepdims=True)\n",
    "    \n",
    "    #regularisation term\n",
    "    for key in grads:\n",
    "        if \"W\" in key:\n",
    "            grads[key]+= (lambd/m)*parameters[key[1:]] \n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-vlDaeQzWA4"
   },
   "outputs": [],
   "source": [
    "def loss_function(y_pred,y, parameters, lambd):\n",
    "    m = y.shape[0]\n",
    "    cost = (-1/m)*np.sum(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "    \n",
    "    regularisation_term = 0\n",
    "    for key in parameters:\n",
    "        if \"W\" in key:\n",
    "            regularisation_term += np.sum(np.square(parameters[key]))\n",
    "    \n",
    "    regularised_cost = cost + (lambd/(2*m))*regularisation_term\n",
    "    \n",
    "    return regularised_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQG2ZymdzWA8"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y):\n",
    "    preds = np.rint(y_pred) #round to int\n",
    "    return np.mean(np.equal(preds,y).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gUcRhv_bJxAJ"
   },
   "outputs": [],
   "source": [
    "def backprop_checker(parameters,grads, x, y,lambd):\n",
    "    epsilon = 1e-7\n",
    "    rel_threshold = 1e-4\n",
    "    num_sample = 10\n",
    "    flag = True #if backprop is correct\n",
    "    print(\"Checking gradients...\")\n",
    "    for param in parameters.keys(): \n",
    "        print(\"Checking: \" + param)\n",
    "        dims = parameters[param].shape\n",
    "        \n",
    "        \n",
    "        num_grad = 0\n",
    "        backprop_grad = 0\n",
    "        \n",
    "        for _ in range(num_sample): #sample 10 neurons\n",
    "            idx = np.zeros(len(dims))\n",
    "            for i in range(len(dims)):\n",
    "                idx[i] = np.random.randint(0,dims[i])\n",
    "            idx = tuple(idx.astype(int))\n",
    "\n",
    "            parameters[param][idx]= parameters[param][idx] + epsilon\n",
    "            y_pred_plus = forward_prop(x,parameters)[0]\n",
    "            J_plus = loss_function(y_pred_plus,y,parameters,lambd)\n",
    "            parameters[param][idx]= parameters[param][idx] - 2*epsilon\n",
    "\n",
    "            y_pred_minus = forward_prop(x,parameters)[0]\n",
    "\n",
    "            J_minus = loss_function(y_pred_minus,y,parameters,lambd)\n",
    "            parameters[param][idx]= parameters[param][idx]+ epsilon\n",
    "\n",
    "            num_grad += (J_plus-J_minus)/(2*epsilon)\n",
    "            backprop_grad += grads[\"d\"+param][idx]\n",
    "            \n",
    "            \n",
    "        num_grad/=num_sample\n",
    "        backprop_grad/=num_sample\n",
    "        rel_error = abs((num_grad-backprop_grad)/num_grad)\n",
    "        if rel_error>rel_threshold:\n",
    "            #print(\"J_plus: \"+ str(J_plus))\n",
    "            #print(\"J_minus: \"+ str(J_minus))\n",
    "            print(\"Numerical grad:\" + str(num_grad))\n",
    "            print(\"Backprop grad:\" + str(backprop_grad))\n",
    "            print(\"Relative error: \" + str(rel_error))\n",
    "            flag = False\n",
    "            print(\"Backprop calculation incorrect\")\n",
    "    print(\"Gradient check complete\")\n",
    "    assert(flag) #if not the backprop is massively out so kill training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2erCPY3HzWA_"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def train_model(X_train, Y_train, X_dev, Y_dev,num_epochs, batch_size,lambd,learning_rate,num_hidden_units=512, parameters = None ):\n",
    "    train_costs = []\n",
    "    train_evals = []\n",
    "    dev_evals = []\n",
    "    fig, (ax1, ax2,ax3) = plt.subplots(1,3,figsize=(10, 3))\n",
    "    \n",
    "    ax1.set_xlabel('Number of iterations')\n",
    "    ax1.set_ylabel('Error')\n",
    "    ax1.set_title('Training Set Error')\n",
    "    \n",
    "    ax2.set_xlabel('Number of iterations')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training Set Accuracy')\n",
    "    \n",
    "    ax3.set_xlabel('Number of iterations')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_title('Dev Set Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ion()\n",
    "\n",
    "    fig.show()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    if parameters==None:\n",
    "        parameters = initialise_parameters(num_hidden_units, X_train.shape[1])\n",
    "        \n",
    "    for epoch in tqdm_notebook(range(num_epochs),total=num_epochs):\n",
    "        #cycle through the entire training set in batches\n",
    "        for i in tqdm_notebook(range(0,X_train.shape[0]//batch_size), total =X_train.shape[0]//batch_size, desc = \"Minibatch number\"):\n",
    "            \n",
    "            \n",
    "            #get the next minibatch to train on\n",
    "            X_train_minibatch = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            Y_train_minibatch = Y_train[:,i*batch_size:(i+1)*batch_size]\n",
    "            #perform one cycle of forward and backward propagation to get the partial derivatives w.r.t. the weights\n",
    "            #and biases. Calculate the cost - used to monitor training\n",
    "            y_pred, caches = forward_prop(X_train_minibatch,parameters)\n",
    "            minibatch_cost = loss_function(y_pred,Y_train_minibatch,parameters,lambd)\n",
    "            minibatch_grads = backprop(X_train_minibatch,Y_train_minibatch,y_pred,parameters, caches,lambd)\n",
    "            \n",
    "            #check backprop calculations - comment out once satisfied\n",
    "           # backprop_checker(parameters,minibatch_grads, X_train_minibatch, Y_train_minibatch,lambd)\n",
    "            \n",
    "            #update the parameters using gradient descent\n",
    "            for param in parameters.keys():\n",
    "                parameters[param] = parameters[param] - learning_rate*minibatch_grads[\"d\"+param]\n",
    "            \n",
    "            #visualise error\n",
    "            train_costs.append(minibatch_cost)\n",
    "            ax1.plot(train_costs)\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "            \n",
    "            train_eval_metric = accuracy(y_pred,Y_train_minibatch)\n",
    "            train_evals.append(train_eval_metric)\n",
    "            ax2.plot(train_evals)\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "            #periodically output an update on the current cost and performance on the dev set for visualisation\n",
    "            if(i%100 == 0):\n",
    "                print(\"Training set error: \"+ str(minibatch_cost))\n",
    "                print(\"Training set accuracy: \"+ str(train_eval_metric))\n",
    "                y_dev_pred,_ = forward_prop(X_dev,parameters)\n",
    "                dev_eval_metric = accuracy(y_dev_pred,Y_dev)\n",
    "                dev_evals.append(dev_eval_metric)\n",
    "                print(\"Accuracy on dev set: \"+ str(dev_eval_metric))\n",
    "                ax3.plot(dev_evals)\n",
    "                fig.canvas.draw()\n",
    "    print(\"Training complete!\")\n",
    "    #return the trained parameters \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 13287
    },
    "colab_type": "code",
    "id": "O-hfbRwgzWBE",
    "outputId": "38d265a1-f726-43ce-99fe-042876ca9704"
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "\n",
    "E = np.random.rand(vocab_size,embedding_size) #embedding matrix\n",
    "\n",
    "def preprocess_input(X,Y, E=E, max_length=max_length):\n",
    "    X_processed = np.zeros((X.shape[0], E.shape[1], max_length))\n",
    "    for i in range(X.shape[0]):\n",
    "        x_clipped = X[i][:max_length]\n",
    "        X_processed[i,:,:len(x_clipped)] = E[x_clipped, :].T\n",
    "    Y_processed = np.expand_dims(Y, axis=0)\n",
    "    return X_processed,Y_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_context_target(X):\n",
    "    i = np.random.randint(X.shape[0])\n",
    "    j = np.random.randint(len(X[i]))\n",
    "    \n",
    "    context = X[i][j]\n",
    "    target = np.random.choice(X[i][max(0,(j-3)):(j+3)])\n",
    "    \n",
    "    return context, target\n",
    "\n",
    "def negative_targets(k):\n",
    "    return np.random.choice(list(imdb.get_word_index().values()), size=k)\n",
    "\n",
    "def relu(x, deriv=True):\n",
    "    if deriv:\n",
    "        return x>0\n",
    "    else:\n",
    "        return np.multiply(x,x>0)\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def sampled_softmax(E,k, X_train,num_iters=int(5000), learning_rate=1e-3):\n",
    "    \n",
    "    W_em1 = np.random.rand(256,embedding_size)\n",
    "    b_em1 = np.random.rand(256,1)\n",
    "    W_em2 = np.random.rand(vocab_size, 256)\n",
    "    b_em2 = np.random.rand(vocab_size, 1)\n",
    "    \n",
    "    train_costs = []\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10, 4))\n",
    "    \n",
    "    ax.set_xlabel('100s of iterations')\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.set_title('Training Set Error')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ion()\n",
    "\n",
    "    fig.show()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    cost=0\n",
    "    \n",
    "    for i in tqdm_notebook(range(num_iters), total=num_iters):\n",
    "        \n",
    "        context, target = sample_context_target(X_train)\n",
    "        neg_targets =  negative_targets(k)\n",
    "\n",
    "        samples = [target] + list(neg_targets)\n",
    "        emb = E[[context],:].T  \n",
    "        z1 = W_em1.dot(emb)+b_em1\n",
    "        a1 = relu(z1)\n",
    "        z2 = W_em2[samples,:].dot(a1)+b_em2[samples]\n",
    "        y_pred = sigmoid(z2)\n",
    "\n",
    "        y = np.zeros_like(y_pred)\n",
    "        y[0,:] = 1\n",
    "        samples = [target] + list(neg_targets)\n",
    "        \n",
    "        cost += -1* (np.sum(np.log(y_pred[0])) + np.sum(np.log(1-y_pred[1:])))\n",
    "    \n",
    "        #visualise error\n",
    "        if i%100==0:\n",
    "            print(cost/100)\n",
    "            print(y_pred)\n",
    "            print(print_review([context]))\n",
    "            print(print_review(samples))\n",
    "            train_costs.append(cost/100)\n",
    "            ax.plot(train_costs)\n",
    "            fig.canvas.draw()\n",
    "            cost=0\n",
    "        \n",
    "        \n",
    "        dz2 = y_pred - y\n",
    "        db_em2 = dz2\n",
    "        dW_em2= dz2.dot(a1.T)\n",
    "\n",
    "        da1 = W_em2[samples,:].T.dot(dz2)\n",
    "        dz1 = da1*relu(z1,deriv=True)\n",
    "\n",
    "        db_em1 = dz1\n",
    "        dW_em1 = dz1.dot(emb.T)\n",
    "        demb = W_em1.T.dot(dz1)\n",
    "\n",
    "        W_em2[samples] -= learning_rate*dW_em2\n",
    "        b_em2[samples] -= learning_rate*db_em2\n",
    "\n",
    "        W_em1 -= learning_rate*dW_em1\n",
    "        b_em1 -= learning_rate*db_em1\n",
    "\n",
    "        E[[context],:]  -= learning_rate*demb.T\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = sampled_softmax(E,15, X_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a, b =preprocess_input(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_model(a,b,a[:200],b[:,:200],num_epochs=1, batch_size=128,lambd=0,learning_rate=1e-3,num_hidden_units=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "E_embedded = TSNE(n_components=2).fit_transform(E[:1000]) #see if any clusters in first 1000 words of vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.scatter(E_embedded[:,0],E_embedded[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
